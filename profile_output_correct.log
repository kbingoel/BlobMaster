GPU: NVIDIA GeForce RTX 4060


================================================================================
PHASE 0: Sessions 1+2 Configuration Validation
================================================================================

================================================================================
PROFILING SESSIONS 1+2 OPTIMIZED CONFIGURATION
================================================================================
  Workers: 32
  Games: 50
  Cards: 5
  MCTS: 3 det Ã— 30 sims (Medium)
  Batching: Enabled (512 batch size, 10ms timeout)
  Parallel Expansion: Enabled (batch_size=30)
  Threading: Disabled (multiprocessing)
================================================================================

Warm-up (2 games)...
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
Running timed test (50 games)...

================================================================================
RESULTS
================================================================================
  Total time: 42.05s
  Games/min: 71.34
  Examples generated: 1200

Batch Evaluator Stats:
  Total evaluations: 0
  Total batches: 0
  Avg batch size: 0.0

Performance vs Expected (75.85 games/min):
  Ratio: 0.94x
================================================================================

================================================================================
PHASE 1: cProfile Analysis (Sessions 1+2 Config)
================================================================================
================================================================================
Profiling Self-Play with cProfile
  Workers: 32
  Games: 10
  Cards: 5
  Batched: True
  Threads: False
  Parallel Expansion: True
  Parallel Batch Size: 30
================================================================================

Starting profiling...
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
Generated 240 examples

================================================================================
TOP 30 FUNCTIONS BY CUMULATIVE TIME
================================================================================
         103306 function calls (96765 primitive calls) in 14.579 seconds

   Ordered by: cumulative time
   List reduced from 292 to 30 due to restriction <30>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     1302    0.005    0.000   14.620    0.011 /home/kbuntu/Documents/Github/BlobMaster/ml/mcts/batch_evaluator.py:341(_collect_batch)
      972    0.002    0.000   14.576    0.015 /usr/local/lib/python3.14/multiprocessing/connection.py:1151(wait)
       14    0.000    0.000   14.568    1.041 /usr/local/lib/python3.14/multiprocessing/pool.py:500(_wait_for_updates)
      972    0.000    0.000   14.543    0.015 /usr/local/lib/python3.14/selectors.py:385(select)
      3/1    0.000    0.000   14.533   14.533 /usr/local/lib/python3.14/threading.py:1029(_bootstrap)
        1    0.000    0.000   14.533   14.533 /usr/local/lib/python3.14/multiprocessing/pool.py:506(_handle_workers)
   1309/4    0.006    0.000   14.531    3.633 /usr/local/lib/python3.14/threading.py:337(wait)
  5234/16   12.993    0.002   14.531    0.908 {method 'acquire' of '_thread.lock' objects}
      2/1    0.000    0.000   14.529   14.529 /home/kbuntu/Documents/Github/BlobMaster/ml/training/selfplay.py:588(generate_games)
      2/1    0.000    0.000   14.526   14.526 /home/kbuntu/Documents/Github/BlobMaster/ml/training/selfplay.py:625(_generate_games_multiprocess)
        1    0.000    0.000   14.526   14.526 /usr/local/lib/python3.14/multiprocessing/pool.py:369(starmap)
        1    0.001    0.001   14.526   14.526 /usr/local/lib/python3.14/multiprocessing/pool.py:767(get)
     1302    0.008    0.000   13.036    0.010 /usr/local/lib/python3.14/queue.py:180(get)
       10    0.000    0.000    1.602    0.160 /usr/local/lib/python3.14/multiprocessing/connection.py:208(send)
      107    0.000    0.000    1.596    0.015 /usr/local/lib/python3.14/multiprocessing/connection.py:423(_send_bytes)
      117    0.001    0.000    1.596    0.014 /usr/local/lib/python3.14/multiprocessing/connection.py:397(_send)
     1303    1.392    0.001    1.396    0.001 {built-in method time.sleep}
       32    0.000    0.000    0.083    0.003 /usr/local/lib/python3.14/multiprocessing/process.py:110(start)
       32    0.000    0.000    0.078    0.002 /usr/local/lib/python3.14/multiprocessing/context.py:297(_Popen)
       32    0.000    0.000    0.068    0.002 /usr/local/lib/python3.14/multiprocessing/popen_forkserver.py:33(__init__)
        1    0.000    0.000    0.045    0.045 /usr/local/lib/python3.14/multiprocessing/pool.py:305(_repopulate_pool)
        1    0.000    0.000    0.045    0.045 /usr/local/lib/python3.14/multiprocessing/pool.py:314(_repopulate_pool_static)
      193    0.042    0.000    0.042    0.000 {built-in method posix.write}
       32    0.000    0.000    0.036    0.001 /usr/local/lib/python3.14/multiprocessing/popen_fork.py:16(__init__)
       32    0.000    0.000    0.036    0.001 /usr/local/lib/python3.14/multiprocessing/popen_forkserver.py:41(_launch)
  214/212    0.000    0.000    0.035    0.000 /usr/local/lib/python3.14/multiprocessing/connection.py:406(_recv)
  214/212    0.034    0.000    0.034    0.000 {built-in method posix.read}
       75    0.006    0.000    0.028    0.000 {method 'dump' of '_pickle.Pickler' objects}
       11    0.001    0.000    0.027    0.002 /usr/local/lib/python3.14/multiprocessing/reduction.py:48(dumps)
      850    0.006    0.000    0.025    0.000 /home/kbuntu/Documents/Github/BlobMaster/venv/lib/python3.14/site-packages/torch/multiprocessing/reductions.py:223(reduce_tensor)




================================================================================
TOP 30 FUNCTIONS BY TOTAL TIME
================================================================================
         103306 function calls (96765 primitive calls) in 14.579 seconds

   Ordered by: internal time
   List reduced from 292 to 30 due to restriction <30>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  5234/16   12.993    0.002   14.531    0.908 {method 'acquire' of '_thread.lock' objects}
     1303    1.392    0.001    1.396    0.001 {built-in method time.sleep}
      193    0.042    0.000    0.042    0.000 {built-in method posix.write}
  214/212    0.034    0.000    0.034    0.000 {built-in method posix.read}
       32    0.013    0.000    0.013    0.000 {built-in method posix.readinto}
      971    0.010    0.000    0.010    0.000 {method 'poll' of 'select.poll' objects}
     1302    0.008    0.000   13.036    0.010 /usr/local/lib/python3.14/queue.py:180(get)
      850    0.007    0.000    0.007    0.000 {method '_share_cuda_' of 'torch._C.StorageBase' objects}
      850    0.006    0.000    0.025    0.000 /home/kbuntu/Documents/Github/BlobMaster/venv/lib/python3.14/site-packages/torch/multiprocessing/reductions.py:223(reduce_tensor)
   1309/4    0.006    0.000   14.531    3.633 /usr/local/lib/python3.14/threading.py:337(wait)
       75    0.006    0.000    0.028    0.000 {method 'dump' of '_pickle.Pickler' objects}
     1302    0.005    0.000   14.620    0.011 /home/kbuntu/Documents/Github/BlobMaster/ml/mcts/batch_evaluator.py:341(_collect_batch)
     1311    0.004    0.000    0.004    0.000 {method 'release' of '_thread.lock' objects}
       32    0.002    0.000    0.002    0.000 {method 'recv' of '_socket.socket' objects}
      972    0.002    0.000   14.576    0.015 /usr/local/lib/python3.14/multiprocessing/connection.py:1151(wait)
      850    0.002    0.000    0.005    0.000 /home/kbuntu/Documents/Github/BlobMaster/venv/lib/python3.14/site-packages/torch/_tensor.py:313(_typed_storage)
      850    0.001    0.000    0.003    0.000 /home/kbuntu/Documents/Github/BlobMaster/venv/lib/python3.14/site-packages/torch/multiprocessing/reductions.py:81(__setitem__)
        1    0.001    0.001   14.526   14.526 /usr/local/lib/python3.14/multiprocessing/pool.py:767(get)
     1434    0.001    0.000    0.003    0.000 /usr/local/lib/python3.14/selectors.py:238(register)
     6472    0.001    0.000    0.001    0.000 {built-in method builtins.len}
      850    0.001    0.000    0.002    0.000 /home/kbuntu/Documents/Github/BlobMaster/venv/lib/python3.14/site-packages/torch/storage.py:693(__new__)
      117    0.001    0.000    1.596    0.014 /usr/local/lib/python3.14/multiprocessing/connection.py:397(_send)
     2606    0.001    0.000    0.002    0.000 /usr/local/lib/python3.14/queue.py:267(_qsize)
     5824    0.001    0.000    0.001    0.000 {built-in method time.monotonic}
       11    0.001    0.000    0.027    0.002 /usr/local/lib/python3.14/multiprocessing/reduction.py:48(dumps)
       75    0.001    0.000    0.001    0.000 /usr/local/lib/python3.14/multiprocessing/reduction.py:38(__init__)
     1434    0.001    0.000    0.004    0.000 /usr/local/lib/python3.14/selectors.py:340(register)
       32    0.001    0.000    0.017    0.001 /usr/local/lib/python3.14/multiprocessing/forkserver.py:388(read_signed)
     1307    0.001    0.000    0.002    0.000 /usr/local/lib/python3.14/threading.py:325(_acquire_restore)
     1311    0.001    0.000    0.001    0.000 /usr/local/lib/python3.14/threading.py:316(__exit__)




Profile saved to: profile_w32_g10_c5.prof
View with: python -m pstats profile_w32_g10_c5.prof

================================================================================
PHASE 2: Manual Timing Profile (Compare Configs)
================================================================================

================================================================================
Manual Timing Profile
================================================================================

Testing: Direct (no batching, threads)
  Time: 114.45s
  Games/min: 5.2
  Examples: 240

Testing: Batched (threads)
  Time: 120.39s
  Games/min: 5.0
  Examples: 240
  Avg batch size: 4.7

Testing: Batched (processes)
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
  Time: 27.53s
  Games/min: 21.8
  Examples: 240
  Avg batch size: 0.0

Testing: Sessions 1+2 Optimized (batched + parallel expansion)
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
[Worker 0] Network on device: cuda:0 (requested: cuda)
[Worker 0] GPU: NVIDIA GeForce RTX 4060
  Time: 19.65s
  Games/min: 30.5
  Examples: 240
  Avg batch size: 0.0

================================================================================
CONFIGURATION COMPARISON
================================================================================

Configuration                  Games/min       Speedup   
------------------------------------------------------------
Direct (no batching, threads)  5.2             1.00x
Batched (threads)              5.0             0.95x
Batched (processes)            21.8            4.16x
Sessions 1+2 Optimized (batched + parallel expansion) 30.5            5.82x

================================================================================
PHASE 3: Batch Evaluator Overhead
================================================================================

================================================================================
Batch Evaluator Overhead Analysis
================================================================================

Test 1: Direct network calls (baseline)
  1000 calls: 1.036s
  Per call: 1.036ms
  Calls/sec: 965

Test 2: Through BatchedEvaluator (single thread)
  1000 calls: 11.349s
  Per call: 11.349ms
  Calls/sec: 88
  Avg batch size: 1.0
  Total batches: 1000

  Overhead: 995.5%

Test 3: Multiple threads (4) through BatchedEvaluator
  1000 calls (4 threads): 2.880s
  Per call: 2.880ms
  Calls/sec: 347
  Avg batch size: 4.0
  Total batches: 250

  Speedup vs direct: 0.36x

================================================================================
Profiling complete!
================================================================================
